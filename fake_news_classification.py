# -*- coding: utf-8 -*-
"""Fake News Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sET0KlkyVnzgZQuwLb9HLoyH4vkW2Zdq

# **Fake News Classification**

In this project, we aim to build a machine learning model to classify news articles as real or fake. We will use a dataset that features news articles from 2015 to 2018. The dataset includes various features that can be useful for this task, such as the text of the article and the source of the article.

Our goal is to train a model that can accurately classify a given news article as real or fake based on these features. This could be a valuable tool for news organizations, social media platforms, and the general public in the fight against fake news.

#Importing Libraries
"""

!pip install scikeras
from google.colab import drive
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud,STOPWORDS
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize,sent_tokenize
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from bs4 import BeautifulSoup
import re,string,unicodedata
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing import text,sequence
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.model_selection import train_test_split
from string import punctuation
from nltk import pos_tag
from nltk.corpus import wordnet
import keras
from keras.models import Sequential
from keras.layers import Dense,Embedding,LSTM,Dropout
from keras.callbacks import ReduceLROnPlateau
from scikeras.wrappers import KerasClassifier, KerasRegressor
from sklearn.model_selection import GridSearchCV
import tensorflow as tf

from google.colab import drive
drive.mount('/content/drive')
import warnings
warnings.filterwarnings('ignore')

# Importing the dataset
fake_df = pd.read_csv('/content/drive/My Drive/AI Final Project/Fake.csv')
real_df = pd.read_csv('/content/drive/My Drive/AI Final Project/True.csv')

"""#Data Summary"""

fake_df.head(10)

real_df.head(10)

fake_df.shape

real_df.shape

# Create a summary of dataset attributes
items = [
    [
        col,
        fake_df[col].dtype,
        fake_df[col].nunique(),
        list(fake_df[col].unique()[:3]),
        fake_df[col].isnull().sum()
    ] for col in fake_df
]

# Display the summary in a DataFrame
display(pd.DataFrame(data=items, columns=[
    'Attributes',
    'Data Type',
    'Total Unique',
    'Unique Sample',
    'Total Missing'
]))

# Create a summary of dataset attributes
items = [
    [
        col,
        real_df[col].dtype,
        real_df[col].nunique(),
        list(real_df[col].unique()[:3]),
        real_df[col].isnull().sum()
    ] for col in real_df
]

# Display the summary in a DataFrame
display(pd.DataFrame(data=items, columns=[
    'Attributes',
    'Data Type',
    'Total Unique',
    'Unique Sample',
    'Total Missing'
]))

"""#Data Preprocessing

##Data Cleaning
"""

fake_df

real_df.head(10)

df = pd.concat([real_df, fake_df]).reset_index(drop = True) #Merging the 2 datasets

"""**Difference in text**: Under the text column, the beginning of articles in the Real news dataset has a source, unlike the Fake news dataset. They will be removed since they could mislead the model.
Some of the text are also tweets instead of news articles.
"""

df

df.info()

df.isna().sum() # Checking for nan Values

df.title.count()

df.subject.value_counts()

"""There are different news subjects/categories in real and fake datasets. Hence, they must be excluded.

In our dataset, each news article is expected to start with the name of the source (or publisher), followed by a hyphen, and then the text of the article. However, some articles do not follow this format.

The code block below is used to identify such articles. It iterates over each article and tries to split the text into two parts at the first occurrence of " -". If the text cannot be split into two parts, or if the first part (assumed to be the source) is 260 characters or longer (else it's possible it is a tweet), the article is assumed to not have a source.

The indices of such articles are stored in the `unknown_publishers` list for further processing.
"""

unknown_publishers = []
for index,row in enumerate(real_df.text.values):
    try:
        record = row.split(" -", maxsplit=1)

        record[1]
        assert(len(record[0]) < 260)
    except:
        unknown_publishers.append(index)

real_df.iloc[unknown_publishers].text

real_df.iloc[8970]
#index 8970 has no text

publishers = []
texts = []

for index, row in enumerate(real_df.text.values):
    if index in unknown_publishers:
        # If the article does not have a source, append the entire text to `texts`
        # and append "Unknown" to `publishers`
        texts.append(row)
        publishers.append("Unknown")
    else:
        # If the article has a source, split the text into source and article text
        source, text = row.split(" -", maxsplit=1)
        publishers.append(source)
        texts.append(text)

real_df["publisher"] = publishers
real_df["text"] = texts

del publishers, texts, record, unknown_publishers

#checking for rows with empty text like row: 8970
[index for index,text in enumerate(real_df.text.values) if str(text).strip() == '']

real_df.drop(8970, axis=0, inplace=True)

real_df.head(10)

# checking for empty rows in fake news
empty_fake_index = [index for index,text in enumerate(fake_df.text.values) if str(text).strip() == '']
print(f"No of empty rows: {len(empty_fake_index)}")
fake_df.iloc[empty_fake_index].tail()

fake_df['category'] = 0
real_df['category'] = 1

fake_df

real_df

"""## Text Preprocessing"""

#Combining Title and Text
real_df["text"] = real_df["title"] + " " + real_df["text"]
fake_df["text"] = fake_df["title"] + " " + fake_df["text"]

# Dropping subject because it differs in both datasets
# Dropping date, title and publisher
real_df_new = real_df.drop(["subject", "date","title",  "publisher"], axis=1)
fake_df_new = fake_df.drop(["subject", "date", "title"], axis=1)

df = real_df_new.append(fake_df_new, ignore_index=True)
del real_df_new, fake_df_new

df

#import matplotlib.pyplot as plt

# Count the number of instances in each category
category_counts = df['category'].value_counts()

print(category_counts)

# Plot the counts
plt.figure(figsize=(10,6))
plt.bar(category_counts.index, category_counts.values, alpha=0.8)
plt.title('Category Counts')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Category', fontsize=12)
plt.show()

"""#Exploratory Data Analysis"""

chart=sns.countplot(x='category',data=df,palette='Blues_r')
plt.title("Fake VS Real",
          fontsize='20',
          backgroundcolor='aliceblue',
          color='blue');

plt.figure(figsize = (8,8))
sns.countplot(y = 'subject',data = fake_df)

plt.figure(figsize = (8,8))
sns.countplot(y = 'subject',data = real_df)

nltk.download('stopwords')
stop = set(stopwords.words('english'))
punctuation = list(string.punctuation)
stop.update(punctuation)



# plot the word cloud for text that is real
plt.figure(figsize = (20,20))
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop).generate(" ".join(df[df.category == 1].text))
plt.imshow(wc, interpolation = 'bilinear')

# plot the word cloud for text that is fake
plt.figure(figsize = (20,20))
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop).generate(" ".join(df[df.category == 0].text))
plt.imshow(wc, interpolation = 'bilinear')

"""**N-Gram Analysis**"""

texts = ' '.join(df['text'])

string = texts.split(" ")

def draw_n_gram(string,i):
    n_gram = (pd.Series(nltk.ngrams(string, i)).value_counts())[:15]
    n_gram_df=pd.DataFrame(n_gram)
    n_gram_df = n_gram_df.reset_index()
    n_gram_df = n_gram_df.rename(columns={"index": "word", 0: "count"})
    print(n_gram_df.head())
    plt.figure(figsize = (16,9))
    return sns.barplot(x='count',y='word', data=n_gram_df)

"""Unigram Analysis"""

draw_n_gram(string,1)

draw_n_gram(string,2)

draw_n_gram(string,3)

"""#More Data Preprocessing"""

#Removing the html strips
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

#Removing the square brackets
def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)

# Removing URL's
def remove_urls(text):
    return re.sub(r'http\S+', '', text)

#Removing emoji
def remove_emoji(data):
    emoji_clean= re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    data=emoji_clean.sub(r'',data)
    url_clean= re.compile(r"https://\S+|www\.\S+")
    data=url_clean.sub(r'',data)
    return data

#Removing special characters
def remove_special_characters(text, remove_digits=True):
    pattern=r'[^a-zA-z0-9\s]'
    text=re.sub(pattern,'',text)
    return text

#Removing the noisy text
def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    text = remove_urls(text)
    text = remove_emoji(text)
    text = remove_special_characters(text)

    return text

#Apply function on review column
df['text']=df['text'].apply(denoise_text)

df

nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
print(stop_words)

def preprocess_text(text):
    text = re.sub(r'\W', ' ', str(text))
    text = text.lower()
    text = re.sub(r'^br$', ' ', text)
    text = re.sub(r'\s+br\s+',' ',text)
    text = re.sub(r'\s+[a-z]\s+', ' ',text)
    text = re.sub(r'^b\s+', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])

    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:
            result.append(token)

    return result

df['text'] = df['text'].apply(preprocess_text)

df

df['text'] = df['text'].apply(lambda x: " ".join(x))
df.head()

"""#Feature Engineering

"""

X = df['text']
y = df['category']

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)

tokenizer = Tokenizer(num_words=1000)
tokenizer.fit_on_texts(X_train)
tokenized_train = tokenizer.texts_to_sequences(X_train)
X_train = pad_sequences(tokenized_train, maxlen=300)

tokenized_test = tokenizer.texts_to_sequences(X_temp)
X_temp = sequence.pad_sequences(tokenized_test, maxlen=300)

X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

"""#Model Training, Test and Evaluation

##LSTM
"""

def create_model(dropout_rate=0.2, recurrent_dropout_rate=0.2):
    model = Sequential()
    model.add(Embedding(input_dim=5000, output_dim=32))
    model.add(LSTM(32, dropout=dropout_rate, recurrent_dropout=recurrent_dropout_rate))
    model.add(Dense(units = 32 , activation = 'relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)
model = KerasClassifier(build_fn=create_model, epochs=3, batch_size=64, verbose=1, dropout_rate=0.0, recurrent_dropout_rate=0.0)

param_grid = {
    'epochs': [3, 5],
    'batch_size': [32, 64]
}

grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, error_score='raise')
# Train the model with validation data
grid_result = grid.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=[early_stop])

# Convert to DataFrame
cv_results_df = pd.DataFrame(grid_result.cv_results_)

# Print the DataFrame
print(cv_results_df)

"""The grid search results show that the best model was obtained with a batch size of 32 and 3 epochs, achieving a mean test score of approximately 0.974."""

lstm_model = create_model(dropout_rate=0.2, recurrent_dropout_rate=0.2)
history = lstm_model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stop])

# Plot training & validation accuracy values
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()

# Evaluate the model
loss, accuracy = lstm_model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')
print(f'Test Accuracy: {accuracy}')

from sklearn.metrics import roc_auc_score

# Use the model to predict the probabilities of the positive class
y_pred_proba = lstm_model.predict(X_test)

# Calculate the AUC score
auc = roc_auc_score(y_test, y_pred_proba)

# Print the AUC score
print(auc)

#import pickle
#saved_model_lstm = pickle.dumps(lstm_model)

#with open('model_lstm.pkl', 'wb') as file:
#    file.write(saved_model_lstm)


#from google.colab import files

# Download the file to your computer
#files.download('model_lstm.pkl')

# Save the tokenizer
#with open('tokenizer_keras.pkl', 'wb') as handle:
#    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

#files.download('tokenizer_keras.pkl')

from sklearn.metrics import accuracy_score, classification_report


lstm_predictions = lstm_model.predict(X_test)
lstm_predictions = (lstm_predictions > 0.5).astype(int)  # Adjust for binary classification

# Evaluate LSTM performance
accuracy_lstm = accuracy_score(y_test, lstm_predictions)
classification_report_lstm = classification_report(y_test, lstm_predictions)

# Confusion Matrix
conf_matrix_lstm = confusion_matrix(y_test, lstm_predictions)

# ROC Curve

fpr_lstm, tpr_lstm, _ = roc_curve(y_test, lstm_predictions)
roc_auc_lstm = auc(fpr_lstm, tpr_lstm)

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr_lstm, tpr_lstm, label=f'LSTM (AUC = {roc_auc_lstm:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

"""##BERT"""

!pip install transformers
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from torch.utils.data import DataLoader, TensorDataset, random_split
import torch

X = df['text']
y = df['category']

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

X_train

X_temp

X_val

# Tokenize and prepare training and validation data
tokenized_train = tokenizer(X_train.tolist(), padding=True, truncation=True, return_tensors="pt")
tokenized_val = tokenizer(X_val.tolist(), padding=True, truncation=True, return_tensors="pt")

# Create DataLoader for training and validation data
train_dataset = TensorDataset(tokenized_train['input_ids'], tokenized_train['attention_mask'], torch.tensor(y_train.values))
val_dataset = TensorDataset(tokenized_val['input_ids'], tokenized_val['attention_mask'], torch.tensor(y_val.values))

train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)

# Load pre-trained BERT model and create a model for sequence classification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Define optimizer
optimizer = AdamW(model.parameters(), lr=2e-5)

# Training loop
num_epochs = 3
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Initialize an empty list to store validation losses
val_losses = []

# Training loop with validation
for epoch in range(num_epochs):
    model.train()
    for batch in train_dataloader:
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

    # Validation loop
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for batch in val_dataloader:
            input_ids, attention_mask, labels = batch
            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            val_loss += outputs.loss.item()

    # Calculate average validation loss for the epoch
    avg_val_loss = val_loss / len(val_dataloader)
    val_losses.append(avg_val_loss)

    print(f"Epoch {epoch + 1}/{num_epochs} - Validation Loss: {avg_val_loss}")

# Plot the validation loss over epochs
plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')
plt.title('Validation Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Validation Loss')
plt.legend()
plt.show()

# Save the model
model.save_pretrained('/content/drive/My Drive/AI Final Project')

# Save the tokenizer
tokenizer.save_pretrained('/content/drive/My Drive/AI Final Project')

import pickle


# Save the trained model as a pickle string.
saved_model_bert = pickle.dumps(model)

# Save the pickled model to a file
with open('modelbert.pkl', 'wb') as file:
    file.write(saved_model_bert)



from google.colab import files

# Download the file to your computer
files.download('modelbert.pkl')

from sklearn.metrics import accuracy_score, classification_report


bert_predictions = model.predict(X_test)
bert_predictions = (bert_predictions > 0.5).astype(int)  # Adjust for binary classification

# Evaluate BERT performance
accuracy_bert = accuracy_score(y_test, bert_predictions)
classification_report_bert = classification_report(y_test, bert_predictions)

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, roc_curve, auc

# Confusion Matrix
conf_matrix_bert = confusion_matrix(y_test, bert_predictions)
conf_matrix_lstm = confusion_matrix(y_test, lstm_predictions)

# ROC Curve
fpr_bert, tpr_bert, _ = roc_curve(y_test, bert_predictions)
roc_auc_bert = auc(fpr_bert, tpr_bert)

fpr_lstm, tpr_lstm, _ = roc_curve(y_test, lstm_predictions)
roc_auc_lstm = auc(fpr_lstm, tpr_lstm)

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr_bert, tpr_bert, label=f'BERT (AUC = {roc_auc_bert:.2f})')
plt.plot(fpr_lstm, tpr_lstm, label=f'LSTM (AUC = {roc_auc_lstm:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()